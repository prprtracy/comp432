{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbQoRopizRybUftxV6rpq1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8Ug3eH3P9J72"},"source":["# **Categorical Cross-Entropy**\n","\n","The *Categorical Cross-Entropy* is an objective function often used in classification problems. It measures \"how far\" the probability over the classes predicted by the machine learning model is from the desired targets. It is defined as:\n","\n","$\n","cross-entropy = -\\frac{1}{N} \\sum_{n=1}^N \\sum_{k=1}^K y_{n,k} log(p_{n,k}),\n","$\n","\n","where N is the number of training examples, and K is the number of classes.  The target value for the example $x_n$ and the class $k$ is indicated as $y_{n,k}$. More precisely, the targets Y are summarized in a matrix $\\mathbb{R}^{N x K}$ where the labels are encoded as one-hot vectors:\n","\n","$ Y = \n","\\begin{bmatrix}\n","1 & 0 & 0\\\\\n","0 & 0 & 1 \\\\\n","0 & 1 & 0 \\\\\n","1 & 0 & 0\n","\\end{bmatrix}\n","$\n","\n","The output probability $p_{n,k}$ summarizes how likely is the class $k$ for the training example $x_n$. More precisely, the output probabilities P are summarized in a matrix $\\mathbb{R}^{N x K}$:\n","\n","$ P = \n","\\begin{bmatrix}\n","0.6 & 0.2 & 0.2\\\\\n","0.3 & 0.3 & 0.4 \\\\\n","0.4 & 0.5 & 0.1 \\\\\n","0.6 & 0.1 & 0.3\n","\\end{bmatrix}\n","$\n","\n","The categorical cross-entropy ranges between 0 (perfect solution) and +inf (worst solution).\n","\n","Let's now define a function to compute the categorical cross-entropy."]},{"cell_type":"code","metadata":{"id":"EFSN5R8MBL8Y","executionInfo":{"status":"ok","timestamp":1671208157083,"user_tz":300,"elapsed":4,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"wMbpxV8fKeVS","executionInfo":{"status":"ok","timestamp":1671208162207,"user_tz":300,"elapsed":156,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}}},"source":["def categorical_cross_entropy(targets, probs):\n","  cross_entropy = (targets * np.log(probs)).sum()\n","  return -cross_entropy/targets.shape[0]"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Let's now assume to have the targets and probabilities outlined above:"],"metadata":{"id":"80kn7jX3NW3_"}},{"cell_type":"code","metadata":{"id":"5vqam9r9EC9c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671208169862,"user_tz":300,"elapsed":182,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}},"outputId":"be1411a4-90c9-41c8-ec93-e1f6c4be89d5"},"source":["targets = np.asarray([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]])\n","probs = np.asarray([[0.6, 0.2, 0.2], [0.3, 0.3, 0.4], [0.4, 0.5, 0.1], [0.6, 0.1, 0.3]])\n","print(categorical_cross_entropy(targets, probs))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6577722899915204\n"]}]},{"cell_type":"markdown","source":["Let's see now what happens if the probabilities are very similar to the targetes:"],"metadata":{"id":"EA7TcpGgRB8m"}},{"cell_type":"code","source":["targets = np.asarray([[0.998, 0.001, 0.001], [0.001, 0.001, 0.998], [0.001, 0.001, 0.998], [0.998, 0.001, 0.001]])\n","print(categorical_cross_entropy(targets, targets))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79m7_t9oRKfp","executionInfo":{"status":"ok","timestamp":1671208171600,"user_tz":300,"elapsed":179,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}},"outputId":"ef467bed-132a-4de4-c2bb-a0f46d90bdef"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["0.015813509223296007\n"]}]},{"cell_type":"markdown","source":["In the case above, the solution is good because the predictions and the targets are very close. The cross-entropy is indeed close to zero.\n","\n","\n","Let's see what happens if the output classes are equiprobable: "],"metadata":{"id":"GoUJXrcbR3hk"}},{"cell_type":"code","source":["targets = np.asarray([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]])\n","probs = np.asarray([[0.333, 0.333, 0.333], [0.333, 0.333, 0.333], [0.333, 0.333, 0.333], [0.333, 0.333, 0.333]])\n","print(categorical_cross_entropy(targets, probs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YtBtAxLLTMp9","executionInfo":{"status":"ok","timestamp":1671208173641,"user_tz":300,"elapsed":155,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}},"outputId":"0b09a8b9-394a-4834-e412-87c6881dd596"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0996127890016931\n"]}]},{"cell_type":"markdown","source":["When the classes are equiprobable, the categorical cross-entropy is -$log(1/k)$. This is already a bad system, but we can do even worse. For instance:"],"metadata":{"id":"EnSpGbFTTYRK"}},{"cell_type":"code","source":["targets = np.asarray([[1, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]])\n","probs = np.asarray([[0.001, 0.998, 0.001], [0.001, 0.998, 0.001], [0.001, 0.001, 0.9998], [0.001, 0.998, 0.001]])\n","print(categorical_cross_entropy(targets, probs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdynnCxBTtQ6","executionInfo":{"status":"ok","timestamp":1671208175623,"user_tz":300,"elapsed":143,"user":{"displayName":"Mirco Ravanelli","userId":"06892056361698510975"}},"outputId":"28702b68-44db-4893-8ab4-d9be86bcb0fa"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["6.907755278982137\n"]}]},{"cell_type":"markdown","source":["In this case, the categorical cross-entropy is much higher because the predictions of the machine learning algorithm are totally wrong (even worse than the random guess). "],"metadata":{"id":"nqmJSnqTUggb"}},{"cell_type":"markdown","source":["**Summary**: Categorical cross-entropy is a popular objective function that can tell us how good a classifier is. It is better than the accuracy because it is a \"soft\" measure able to rank more precisely the solutions explored during training. "],"metadata":{"id":"rvtRTy5AQDui"}}]}